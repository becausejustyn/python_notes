{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM algorithm from scratch\n",
    "# The following code is adapted from https://github.com/VXU1230/Medium-Tutorials/tree/master/em\n",
    "# Rosalind Wang\n",
    "# 30 Sept 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def get_random_psd(n):\n",
    "    x = np.random.normal(0, 1, size=(n, n))\n",
    "    return np.dot(x, x.transpose())\n",
    "\n",
    "\n",
    "def initialize_random_params():\n",
    "    params = {'phi': np.random.uniform(0, 1),\n",
    "              'mu0': np.random.normal(0, 1, size=(2,)),\n",
    "              'mu1': np.random.normal(0, 1, size=(2,)),\n",
    "              'sigma0': get_random_psd(2),\n",
    "              'sigma1': get_random_psd(2)}\n",
    "    return params\n",
    "\n",
    "def e_step(x, params):\n",
    "    # p(y) -- shape (2,)\n",
    "    log_p_y = np.log([1-params[\"phi\"], params[\"phi\"]])\n",
    "    # p(x|y) -- shape (N,2)\n",
    "    log_p_x_y = np.log([stats.multivariate_normal(params[\"mu0\"], params[\"sigma0\"]).pdf(x),\n",
    "            stats.multivariate_normal(params[\"mu1\"], params[\"sigma1\"]).pdf(x)])\n",
    "    \n",
    "    # calculate log(p(x,y))\n",
    "    log_p_xy = log_p_y[np.newaxis, ...] + log_p_x_y.T\n",
    "    # log(\\sum p(x,y))\n",
    "    log_p_xy_norm = logsumexp(log_p_xy, axis=1)\n",
    "    \n",
    "    # So p(y|x) is now: \n",
    "    p_y_x = np.exp(log_p_xy - log_p_xy_norm[..., np.newaxis])\n",
    "    \n",
    "    return log_p_xy_norm, p_y_x\n",
    "\n",
    "def m_step(x, params):\n",
    "    total_count = x.shape[0]\n",
    "    \n",
    "    # get the heuristics of the posterior from the e-step, i.e. p(y|x)\n",
    "    # and calculate the sum(y=c|x) for each class\n",
    "    _, heuristics = e_step(x, params)\n",
    "    heuristic0 = heuristics[:, 0]\n",
    "    heuristic1 = heuristics[:, 1]\n",
    "    sum_heuristic1 = np.sum(heuristic1)\n",
    "    sum_heuristic0 = np.sum(heuristic0)\n",
    "    \n",
    "    # Calculate the new parameters\n",
    "    phi = (sum_heuristic1/total_count)\n",
    "    mu0 = (heuristic0[..., np.newaxis].T.dot(x)/sum_heuristic0).flatten()\n",
    "    mu1 = (heuristic1[..., np.newaxis].T.dot(x)/sum_heuristic1).flatten()\n",
    "    diff0 = x - mu0\n",
    "    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\n",
    "    diff1 = x - mu1\n",
    "    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\n",
    "    params = {'phi': phi, 'mu0': mu0, 'mu1': mu1, 'sigma0': sigma0, 'sigma1': sigma1}\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_avg_log_likelihood(x, params):\n",
    "    loglikelihood, _ = e_step(x, params)\n",
    "    return np.mean(loglikelihood)\n",
    "\n",
    "\n",
    "def run_em(x, params):\n",
    "    avg_loglikelihoods = []\n",
    "    while True:\n",
    "        avg_loglikelihood = get_avg_log_likelihood(x, params)\n",
    "        avg_loglikelihoods.append(avg_loglikelihood)\n",
    "        if len(avg_loglikelihoods) > 2 and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001:\n",
    "            break\n",
    "        params = m_step(x, params)\n",
    "        \n",
    "    print(\"\\tphi: %s\\n\\tmu_0: %s\\n\\tmu_1: %s\\n\\tsigma_0: %s\\n\\tsigma_1: %s\"\n",
    "               % (params['phi'], params['mu0'], params['mu1'], params['sigma0'], params['sigma1']))\n",
    "    _, posterior = e_step(x, params)\n",
    "    forecasts = np.argmax(posterior, axis=1)\n",
    "    return forecasts, posterior, avg_loglikelihoods, params\n",
    "\n",
    "def learn_params(x_labeled, y_labeled):\n",
    "    n = x_labeled.shape[0]\n",
    "    phi = x_labeled[y_labeled == 1].shape[0] / n\n",
    "    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\n",
    "    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\n",
    "    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias= True)\n",
    "    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\n",
    "    return {'phi': phi, 'mu0': mu0, 'mu1': mu1, 'sigma0': sigma0, 'sigma1': sigma1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# to make outputs reproducible across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# create data set\n",
    "nSamples = 100\n",
    "X = 2 * np.random.rand(nSamples,1)\n",
    "y = 4 + 3 * X + np.random.randn(nSamples, 1)\n",
    "\n",
    "# plot data set\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "\n",
    "\n",
    "X_m = np.c_[np.ones((100,1)), X]           # add x0 = 1 to each instance\n",
    "eta = 0.1\n",
    "n_iteration = 100                          # a fixed number of iterations\n",
    "w = np.random.randn(2,1)\n",
    "\n",
    "# to rpedict the regression line from x=0 to 2 \n",
    "X_new = np.array([[0],[2]])                 # min and max x-values\n",
    "X_new_m = np.c_[np.ones((2,1)), X_new]      # add a column 0 with all 1\n",
    "tr = 0.2\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    gradients = 2/nSamples * X_m.T.dot( X_m.dot(w) - y )  \n",
    "    w = w - eta * gradients\n",
    "\n",
    "    if iteration % 25 == 0:      \n",
    "        y_predict = X_new_m.dot(w)    # predicted y values for each X_new value\n",
    "        plt.plot(X_new, y_predict, 'r-', alpha=tr)\n",
    "        tr = min(tr + 0.1, 1.0)\n",
    "\n",
    "y_predict = X_new_m.dot(w)    # predicted y values for each X_new value\n",
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://beckernick.github.io/neural-network-scratch/)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "#simulating data from a multivariate normal dist\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generalized linear models usually tranform a linear model of the predictors by using a [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function). \n",
    "# In logistic regression, the link function is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). We can implement this really easily.\n",
    "\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-Likelihood\n",
    "\n",
    "The log-likelihood can be viewed as as sum over all the training data. Mathematically,\n",
    "\n",
    "$$\\begin{equation}\n",
    "ll = \\sum_{i=1}^{N}y_{i}\\beta ^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})\n",
    "\\end{equation}$$\n",
    "\n",
    "where $y$ is the target class, $x_{i}$ represents an individual data point, and $\\beta$ is the weights vector.\n",
    "\n",
    "I can easily turn that into a function and take advantage of matrix algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Gradient\n",
    "\n",
    "Now I need an equation for the gradient of the log-likelihood. By taking the derivative of the equation above and reformulating in matrix form, the gradient becomes: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\bigtriangledown ll = X^{T}(Y - Predictions)\n",
    "\\end{equation}$$\n",
    "\n",
    "Again, this is really easy to implement. It's so simple I don't even need to wrap it into a function. The gradient here looks very similar to the output layer gradient in a neural network (see my [post](https://beckernick.github.io/neural-network-scratch/) on neural networks if you're curious).\n",
    "\n",
    "This shouldn't be too surprising, since a neural network is basically just a series of non-linear link functions applied after linear manipulations of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in xrange(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with log likelihood gradient\n",
    "        output_error_signal = target - predictions\n",
    "        \n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "\n",
    "        # Print log-likelihood every so often\n",
    "        if step % 10000 == 0:\n",
    "            print log_likelihood(features, target, weights)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the regression\n",
    "\n",
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
