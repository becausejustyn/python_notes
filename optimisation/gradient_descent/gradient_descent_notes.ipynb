{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from typing import List\n",
    "from plotly import graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Optimisation\n",
    "\n",
    "In machine learning, optimisation refers to the process of an algorithm improving previous results to improve the guess. The term `loss` is used to measure how far away the result is from the prediction. The distance between the two values is how we quantify if the algorithm is improving or declining. The most common `loss` functions are\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} = \\sum_{i = 1}^{n} (Y_{i} - \\hat{Y}_{i})^{2}\n",
    "$$\n",
    "\n",
    "In matrix notation,\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n}(e_{i})^{2} = \\frac{1}{n}e^{T}e\n",
    "$$\n",
    "\n",
    "#### Residual Sum of Squares (RSS)/Sum of Squared Errors (SSE)\n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i = 1}^{n} (y_{i} - f(x_{i}))^{2}\n",
    "$$\n",
    "\n",
    "#### Root Mean Square Error (RMSE)\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{\\sum_{i = 1}^{N}(x_{i} - \\hat{x}_{i})^{2}}{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is like a walk down a mountain, where if the next step increases in height, you take a step back and try to find a path that lets you get to the lowest point of the mountain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraboloid(x, y):\n",
    "    return(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs: [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "ys: [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "zs: [[200, 181, 164, 149, 136, 125, 116, 109, 104, 101, 100, 101, 104, 109, 116, 125, 136, 149, 164, 181, 200], [181, 162, 145, 130, 117, 106, 97, 90, 85, 82, 81, 82, 85, 90, 97, 106, 117, 130, 145, 162, 181], [164, 145, 128, 113, 100, 89, 80, 73, 68, 65, 64, 65, 68, 73, 80, 89, 100, 113, 128, 145, 164], [149, 130, 113, 98, 85, 74, 65, 58, 53, 50, 49, 50, 53, 58, 65, 74, 85, 98, 113, 130, 149], [136, 117, 100, 85, 72, 61, 52, 45, 40, 37, 36, 37, 40, 45, 52, 61, 72, 85, 100, 117, 136]] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Data\n",
    "\n",
    "# Test data generation (only really necessary for the plotting below)\n",
    "xs_start = ys_start = -10\n",
    "xs_stop = ys_stop = 11\n",
    "xs_step = ys_step = 1\n",
    "\n",
    "xs: List[float] = [i for i in range(xs_start, xs_stop, xs_step)]\n",
    "ys: List[float] = [i for i in range(ys_start, ys_stop, ys_step)]\n",
    "zs: List[List[float]] = []\n",
    "\n",
    "for x in xs:\n",
    "    temp_res: List[float] = []\n",
    "    for y in ys:\n",
    "        result: float = paraboloid(x, y)\n",
    "        temp_res.append(result)\n",
    "    zs.append(temp_res)\n",
    "\n",
    "print(f'xs: {xs}\\n')\n",
    "print(f'ys: {ys}\\n')\n",
    "print(f'zs: {zs[:5]} ...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the generated test data\n",
    "fig = go.Figure(\n",
    "    go.Surface(\n",
    "        x = xs, y = ys, z = zs, colorscale = 'Viridis'\n",
    "        ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and Derivatives\n",
    "\n",
    "> ...a vector-valued function $f...,$ whose value at a point $p$ is the vector whose components are the partial derivatives of $f$ at $p$.\n",
    "\n",
    "In short, for any given value $p$ on the function, there is a vector of `partial derivatives`, which point in the direction of greatest increase.\n",
    "\n",
    "- a derivative measures the rate of change of a function in respect to the changes in its input.\n",
    "- the difference with partial derivatives is that you're deriving while keeping every other variable as a constant.\n",
    "\n",
    "Using our parabloid example:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} (x^{2} + y^{2}) = 2x\n",
    "$$\n",
    "\n",
    "With those partial derivatives we're now able to compute any gradient for any point $p$ sitting on the plotted surface of function $f$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(vec: List[float]) -> List[float]:\n",
    "    assert len(vec) == 2\n",
    "    x: float = vec[0]\n",
    "    y: float = vec[1]\n",
    "    return [2 * x, 2 * y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the gradient vector is pointing upwards to the direction of greatest **increase**. We need to turn that vector into the opposite direction so that it points to the direction of greatest decrease.\n",
    "\n",
    "We can do that if we multiply the gradient vector by `-1`.\n",
    "\n",
    "The algorithm works as follows.\n",
    "\n",
    "1. Get the starting position $p$ (which is represented as a vector) on $f$\n",
    "2. Compute the gradient at point $p$\n",
    "3. Multiply the gradient by a negative `step size` (usually a value smaller than `1`)\n",
    "4. Compute the next position of $p$ on the surface by adding the rescaled gradient vector to the vector $p$\n",
    "5. Repeat `step 2` with the new $p$ until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = step_size\n",
    "\n",
    "def compute_step(curr_pos: List[float], learning_rate: float) -> List[float]:\n",
    "    grad: List[float] = compute_gradient(curr_pos)\n",
    "    grad[0] *= -learning_rate\n",
    "    grad[1] *= -learning_rate\n",
    "    next_pos: List[float] = [0, 0]\n",
    "    next_pos[0] = curr_pos[0] + grad[0]\n",
    "    next_pos[1] = curr_pos[1] + grad[1]\n",
    "    return(next_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 10]\n"
     ]
    }
   ],
   "source": [
    "# define a random starting position p\n",
    "\n",
    "start_pos: List[float]\n",
    "\n",
    "# Ensure that we don't start at a minimum (0, 0 in our case)\n",
    "while True:\n",
    "    start_x: float = randint(xs_start, xs_stop)\n",
    "    start_y: float = randint(ys_start, ys_stop)\n",
    "    if start_x != 0 and start_y != 0:\n",
    "        start_pos = [start_x, start_y]\n",
    "        break\n",
    "\n",
    "print(start_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we wrap our `compute_step` function into a loop to iteratively walk down the surface and eventually reach a local minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [7.984, 9.98]\n",
      "Epoch 500: [2.9342098587795595, 3.6677623234744514]\n",
      "Epoch 1000: [1.078355147214322, 1.3479439340179038]\n",
      "Epoch 1500: [0.3963076533344121, 0.4953845666680148]\n",
      "Epoch 2000: [0.14564752298642572, 0.1820594037330319]\n",
      "Epoch 2500: [0.05352710393957821, 0.06690887992447267]\n",
      "Epoch 3000: [0.019671813137703987, 0.02458976642212994]\n",
      "Epoch 3500: [0.007229612731553134, 0.009037015914441402]\n",
      "Epoch 4000: [0.0026569640471043837, 0.0033212050588804732]\n",
      "Epoch 4500: [0.0009764641910616871, 0.0012205802388271054]\n",
      "Best guess for a minimum: [0.00035958074166348857, 0.0004494759270793589]\n"
     ]
    }
   ],
   "source": [
    "epochs: int = 5000\n",
    "learning_rate: float = 0.001\n",
    "\n",
    "best_pos: List[float] = start_pos\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    next_pos: List[float] = compute_step(best_pos, learning_rate)\n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch {i}: {next_pos}')\n",
    "    best_pos = next_pos\n",
    "\n",
    "print(f'Best guess for a minimum: {best_pos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
