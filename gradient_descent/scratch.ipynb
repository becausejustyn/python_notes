{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(444)\n",
    "\n",
    "N = 10000\n",
    "sigma = 0.1\n",
    "noise = sigma * np.random.randn(N)\n",
    "x = np.linspace(0, 2, N)\n",
    "d = 3 + 2 * x + noise\n",
    "d.shape = (N, 1)\n",
    "\n",
    "# We need to prepend a column vector of 1s to `x`.\n",
    "X = np.column_stack((np.ones(N, dtype=x.dtype), x))\n",
    "print(X.shape)\n",
    "(10000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "def py_descent(x, d, mu, N_epochs):\n",
    "    N = len(x)\n",
    "    f = 2 / N\n",
    "\n",
    "    # \"Empty\" predictions, errors, weights, gradients.\n",
    "    y = [0] * N\n",
    "    w = [0, 0]\n",
    "    grad = [0, 0]\n",
    "\n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        # Can't use a generator because we need to\n",
    "        # access its elements twice.\n",
    "        err = tuple(i - j for i, j in zip(d, y))\n",
    "        grad[0] = f * sum(err)\n",
    "        grad[1] = f * sum(i * j for i, j in zip(err, x))\n",
    "        w = [i + mu * j for i, j in zip(w, grad)]\n",
    "        y = (w[0] + w[1] * i for i in x)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.959859852416156, 2.0329649630002757]\n",
      "Solve time: 21.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now, use this to find a solution:\n",
    "\n",
    "import time\n",
    "\n",
    "x_list = x.tolist()\n",
    "d_list = d.squeeze().tolist()  # Need 1d lists\n",
    "\n",
    "# `mu` is a step size, or scaling factor.\n",
    "mu = 0.001\n",
    "N_epochs = 10000\n",
    "\n",
    "t0 = time.time()\n",
    "py_w = py_descent(x_list, d_list, mu, N_epochs)\n",
    "t1 = time.time()\n",
    "\n",
    "print(py_w)\n",
    "[2.959859852416156, 2.0329649630002757]\n",
    "\n",
    "print('Solve time: {:.2f} seconds'.format(round(t1 - t0, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.95985985 2.03296496]\n"
     ]
    }
   ],
   "source": [
    "def np_descent(x, d, mu, N_epochs):\n",
    "    d = d.squeeze()\n",
    "    N = len(x)\n",
    "    f = 2 / N\n",
    "\n",
    "    y = np.zeros(N)\n",
    "    err = np.zeros(N)\n",
    "    w = np.zeros(2)\n",
    "    grad = np.empty(2)\n",
    "\n",
    "    for _ in it.repeat(None, N_epochs):\n",
    "        np.subtract(d, y, out=err)\n",
    "        grad[:] = f * np.sum(err), f * (err @ x)\n",
    "        w = w + mu * grad\n",
    "        y = w[0] + w[1] * x\n",
    "    return w\n",
    "\n",
    "np_w = np_descent(x, d, mu, N_epochs)\n",
    "print(np_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.320755106\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "setup = (\"from __main__ import x, d, mu, N_epochs, np_descent;\"\n",
    "         \"import numpy as np\")\n",
    "repeat = 5\n",
    "number = 5  # Number of loops within each repeat\n",
    "\n",
    "np_times = timeit.repeat('np_descent(x, d, mu, N_epochs)', setup=setup,\n",
    "                         repeat=repeat, number=number)\n",
    "\n",
    "print(min(np_times) / number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(gradient, start, learn_rate, n_iter = 50, tolerance = 1e-06):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * gradient(vector)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector))\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06,\n",
    "    dtype=\"float64\"\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Recalculating the difference\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)\n",
    "\n",
    "        # Checking if the absolute difference is small enough\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "\n",
    "        # Updating the values of the variables\n",
    "        vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to considering data types, the code above introduces a few modifications related to type checking and ensuring the use of NumPy capabilities:\n",
    "\n",
    "* Lines 8 and 9 check if gradient is a Python callable object and whether it can be used as a function. If not, then the function will raise a TypeError.\n",
    "\n",
    "* Line 12 sets an instance of numpy.dtype, which will be used as the data type for all arrays throughout the function.\n",
    "\n",
    "* Line 15 takes the arguments x and y and produces NumPy arrays with the desired data type. The arguments x and y can be lists, tuples, arrays, or other sequences.\n",
    "\n",
    "* Lines 16 and 17 compare the sizes of x and y. This is useful because you want to be sure that both arrays have the same number of observations. If they don’t, then the function will raise a ValueError.\n",
    "\n",
    "* Line 20 converts the argument start to a NumPy array. This is an interesting trick: if start is a Python scalar, then it’ll be transformed into a corresponding NumPy object (an array with one item and zero dimensions). If you pass a sequence, then it’ll become a regular NumPy array with the same number of elements.\n",
    "\n",
    "* Line 23 does the same thing with the learning rate. This can be very useful because it enables you to specify different learning rates for each decision variable by passing a list, tuple, or NumPy array to gradient_descent().\n",
    "\n",
    "* Lines 24 and 25 check if the learning rate value (or values for all variables) is greater than zero.\n",
    "\n",
    "* Lines 28 to 35 similarly set n_iter and tolerance and check that they are greater than zero.\n",
    "\n",
    "* Lines 38 to 47 are almost the same as before. The only difference is the type of the gradient array on line 40.\n",
    "\n",
    "* Line 49 conveniently returns the resulting array if you have several decision variables or a Python scalar if you have a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,n_iter=100)\n",
    "\n",
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,n_iter=1000)\n",
    "\n",
    "gradient_descent(gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,n_iter=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minibatches in Stochastic Gradient Descent\n",
    "\n",
    "def sgd(\n",
    "    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,\n",
    "    tolerance=1e-06, dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = -learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd(ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,batch_size=3, n_iter=100_000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum in Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(\n",
    "    gradient, x, y, start, learn_rate=0.1, decay_rate=0.0, batch_size=1,\n",
    "    n_iter=50, tolerance=1e-06, dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the decay rate\n",
    "    decay_rate = np.array(decay_rate, dtype=dtype_)\n",
    "    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n",
    "        raise ValueError(\"'decay_rate' must be between zero and one\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Setting the difference to zero for the first iteration\n",
    "    diff = 0\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = decay_rate * diff - learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Start Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(\n",
    "    gradient, x, y, n_vars=None, start=None, learn_rate=0.1,\n",
    "    decay_rate=0.0, batch_size=1, n_iter=50, tolerance=1e-06,\n",
    "    dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = (\n",
    "        rng.normal(size=int(n_vars)).astype(dtype_)\n",
    "        if start is None else\n",
    "        np.array(start, dtype=dtype_)\n",
    "    )\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the decay rate\n",
    "    decay_rate = np.array(decay_rate, dtype=dtype_)\n",
    "    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n",
    "        raise ValueError(\"'decay_rate' must be between zero and one\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Setting the difference to zero for the first iteration\n",
    "    diff = 0\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = decay_rate * diff - learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd(ssr_gradient, x, y, n_vars=2, learn_rate=0.0001,decay_rate=0.8, batch_size=3, n_iter=100_000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 13:02:56.995166: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0000508"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create needed objects\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "var = tf.Variable(2.5)\n",
    "cost = lambda: 2 + var ** 2\n",
    "\n",
    "# Perform optimization\n",
    "for _ in range(100):\n",
    "    sgd.minimize(cost, var_list=[var])\n",
    "\n",
    "# Extract results\n",
    "var.numpy()\n",
    "\n",
    "cost().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
