{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eed3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a74ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality n</th>\n",
       "      <th>prop n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1640</td>\n",
       "      <td>0.334831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3258</td>\n",
       "      <td>0.665169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality n    prop n\n",
       "0       1640  0.334831\n",
       "1       3258  0.665169"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"~/Downloads/winequality-white.csv\", sep=\";\")\n",
    "df.columns = df.columns.str.lower().str.replace(' ','_')\n",
    "\n",
    "df_binary = df.copy()\n",
    "df_binary['quality'] = [1 if x >= 6 else 0 for x in df_binary['quality']]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'quality n': df_binary['quality'].value_counts().sort_index(),\n",
    "    'prop n': df_binary['quality'].value_counts(normalize=True).sort_index()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea34aa8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape (4898, 1)\n",
      "X shape (4898, 11)\n",
      "X Train set: (3918, 11) y Train set: (3918, 1)\n",
      "X Test set: (980, 11) y Test set: (980, 1)\n"
     ]
    }
   ],
   "source": [
    "#target variable, reshape for matrix multiplication purposes\n",
    "y = df_binary['quality'].to_numpy().reshape([-1, 1])\n",
    "#feature variables\n",
    "X = df.drop(['quality'], axis=1).to_numpy()\n",
    "print('y shape', y.shape)\n",
    "print('X shape', X.shape)\n",
    "\n",
    "#Splitting the data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "print('X Train set:', X_train.shape, 'y Train set:', y_train.shape)\n",
    "print('X Test set:', X_test.shape, 'y Test set:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53af3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_val(x):\n",
    "    mean = sum(x) * (1.0 / len(x))\n",
    "    var = list(map(lambda x: (x - mean) ** 2, x))\n",
    "    sd = (sum(var) * 1.0 / (len(var))) ** 0.5\n",
    "    scaled_values = (x - mean) / sd\n",
    "    return scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64a5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_val(X_train)\n",
    "X_test = scale_val(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a4f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0a107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f624b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9aae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51503f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c30e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb0c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(x, y, lr=0.001):\n",
    "    m_current = b_current = 0\n",
    "    print('starting m, b:', m_current, b_current)\n",
    "    iterations = 10\n",
    "    n = len(x)\n",
    "    learning_rate = lr\n",
    "    for i in range(iterations):\n",
    "        predicted_y = m_current*x + b_current\n",
    "        cost = np.round((1/n) * sum([val**2 for val in (y-predicted_y)]),2)\n",
    "        derivative_m = -(2/n)*sum(x*(x-predicted_y))\n",
    "        derivative_b = -(2/n)*sum(x*(y-predicted_y))\n",
    "        m_current = np.round((m_current - learning_rate * derivative_m), 2)\n",
    "        b_current = np.round((b_current - learning_rate * derivative_b), 2)\n",
    "        print('m {}, b {}, cost {}, iteration {}' .format(m_current, b_current, cost, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d9fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting m, b: 0 0\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 0\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 1\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 2\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 3\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 4\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 5\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 6\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 7\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 8\n",
      "m [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], b [-0. -0.  0. -0. -0. -0. -0. -0.  0.  0.  0.], cost [0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67], iteration 9\n"
     ]
    }
   ],
   "source": [
    "batch_grad_descent(X_train, y_train, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80173a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = weight\n",
    "#b = bias\n",
    "\n",
    "def update_weights(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38106db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf43ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c579cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent\n",
    "#calculates the cost for the given X and y\n",
    "\n",
    "def calculate_cost(theta, x, y):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = np.sum(np.square(predictions-y))/(2*m)\n",
    "    return cost\n",
    "\n",
    "#returns the final theta vector and the array of the cost history\n",
    "def gradient_descent(X, y, theta, learning_rate=0.01, iterations=1000):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        prediction = np.dot(X, theta)\n",
    "        theta -= (1/m)*learning_rate*(X.T.dot((prediction - y)))\n",
    "        theta_history[it,:] = theta.T\n",
    "        cost_history[it] = calculate_cost(theta, X, y)\n",
    "        return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3589b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04753e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec765c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stocashtic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=100):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "\n",
    "    for it in range(iterations):\n",
    "        cost = 0.0\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            X_i = X[rand_ind,:].reshape(1, X.shape[1])\n",
    "            y_i = y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i, theta)\n",
    "            theta -= (1/m)*learning_rate*(X_i.T.dot((prediction - y_i)))\n",
    "            cost += calculate_cost(theta, X_i, y_i)\n",
    "            cost_history[it] = cost\n",
    "\n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76584ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d47e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14f7e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  returns the final theta vector and the array of the cost history\n",
    "\n",
    "def stocashtic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=100, batch_size=20):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    n_batches = int(m/batch_size)\n",
    "\n",
    "    for it in range(iterations):\n",
    "        cost = 0.0\n",
    "        indices = np.random.permumtation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
    "            prediction = np.dot(X_i, theta)\n",
    "            \n",
    "            theta -= (1/m)*learning_rate*(X_i.T.dot((prediction - y_i)))\n",
    "            cost += calculate_cost(theta, X_i, y_i)\n",
    "            cost_history[it] = cost\n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce8432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431dbcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1da7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_gd\n",
    "#Arguments:\n",
    "#    parameters -- python dictionary containing your parameters to be updated:\n",
    "#                    parameters['W' + str(l)] = Wl\n",
    "#                    parameters['b' + str(l)] = bl\n",
    "#    grads -- python dictionary containing your gradients to update each parameters:\n",
    "#                    grads['dW' + str(l)] = dWl\n",
    "#                    grads['db' + str(l)] = dbl\n",
    "#    learning_rate -- the learning rate, scalar.\n",
    "#    \n",
    "#    Returns:\n",
    "#    parameters -- python dictionary containing your updated parameters \n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch\n",
    "\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost.\n",
    "    cost = compute_cost(a, Y)\n",
    "    # Backward propagation.\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18aa459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent:\n",
    "\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916db57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
